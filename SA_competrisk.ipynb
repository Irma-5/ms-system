{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkwpj1UGKsa_"
   },
   "source": [
    "# Установка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gitpython\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, gitdb, gitpython\n",
      "\n",
      "   -------------------------- ------------- 2/3 [gitpython]\n",
      "   -------------------------- ------------- 2/3 [gitpython]\n",
      "   ---------------------------------------- 3/3 [gitpython]\n",
      "\n",
      "Successfully installed gitdb-4.0.12 gitpython-3.1.45 smmap-5.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gitpython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SurvivalEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXg5Bfi-dqtX",
    "outputId": "9df745ee-c1a3-4fc6-dcab-611c2c259d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/soda-inria/hazardous\n",
      "  Cloning https://github.com/soda-inria/hazardous to c:\\users\\pridanova\\appdata\\local\\temp\\104\\pip-req-build-90m3snk8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 2] The system cannot find the file specified while executing command git version\n",
      "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycox in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: torchtuples>=0.2.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pycox) (0.2.2)\n",
      "Requirement already satisfied: feather-format>=0.4.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pycox) (0.4.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pycox) (3.15.1)\n",
      "Requirement already satisfied: numba>=0.44 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pycox) (0.62.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.2 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pycox) (1.4.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pycox) (2.32.5)\n",
      "Requirement already satisfied: py7zr>=0.11.3 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pycox) (1.0.0)\n",
      "Requirement already satisfied: pyarrow>=0.4.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from feather-format>=0.4.0->pycox) (22.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from h5py>=2.9.0->pycox) (1.26.4)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from numba>=0.44->pycox) (0.45.1)\n",
      "Requirement already satisfied: texttable in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from py7zr>=0.11.3->pycox) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.20.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from py7zr>=0.11.3->pycox) (3.23.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from py7zr>=0.11.3->pycox) (1.2.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from py7zr>=0.11.3->pycox) (7.0.0)\n",
      "Requirement already satisfied: pyzstd>=0.16.1 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from py7zr>=0.11.3->pycox) (0.18.0)\n",
      "Requirement already satisfied: pyppmd<1.3.0,>=1.1.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from py7zr>=0.11.3->pycox) (1.2.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from py7zr>=0.11.3->pycox) (1.0.6)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from py7zr>=0.11.3->pycox) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from py7zr>=0.11.3->pycox) (1.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.13.2 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pyzstd>=0.16.1->py7zr>=0.11.3->pycox) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from requests>=2.22.0->pycox) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from requests>=2.22.0->pycox) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from requests>=2.22.0->pycox) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from requests>=2.22.0->pycox) (2025.10.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-learn>=0.21.2->pycox) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-learn>=0.21.2->pycox) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-learn>=0.21.2->pycox) (3.6.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from torchtuples>=0.2.0->pycox) (2.3.3)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from torchtuples>=0.2.0->pycox) (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pandas>=0.24.2->torchtuples>=0.2.0->pycox) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pandas>=0.24.2->torchtuples>=0.2.0->pycox) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (1.17.0)\n",
      "Requirement already satisfied: scikit-survival==0.23.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: ecos in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-survival==0.23.0) (2.0.14)\n",
      "Requirement already satisfied: joblib in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-survival==0.23.0) (1.5.2)\n",
      "Requirement already satisfied: numexpr in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-survival==0.23.0) (2.14.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-survival==0.23.0) (1.26.4)\n",
      "Requirement already satisfied: osqp!=0.6.0,!=0.6.1 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-survival==0.23.0) (1.0.5)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-survival==0.23.0) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-survival==0.23.0) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn<1.6,>=1.4.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-survival==0.23.0) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-learn<1.6,>=1.4.0->scikit-survival==0.23.0) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from osqp!=0.6.0,!=0.6.1->scikit-survival==0.23.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from osqp!=0.6.0,!=0.6.1->scikit-survival==0.23.0) (80.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pandas>=1.0.5->scikit-survival==0.23.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pandas>=1.0.5->scikit-survival==0.23.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pandas>=1.0.5->scikit-survival==0.23.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->scikit-survival==0.23.0) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from jinja2->osqp!=0.6.0,!=0.6.1->scikit-survival==0.23.0) (3.0.2)\n",
      "Requirement already satisfied: scikit-learn==1.4 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-learn==1.4) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-learn==1.4) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-learn==1.4) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from scikit-learn==1.4) (3.6.0)\n",
      "Requirement already satisfied: torch in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from seaborn) (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pridanova\\.conda\\envs\\survive\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/soda-inria/hazardous\n",
    "!pip install scikit-survival==0.23.0\n",
    "!pip install scikit-learn==1.4\n",
    "!pip install torch\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DzedYGwCeJS5"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "# from hazardous import SurvivalBoost\n",
    "# from hazardous.metrics import integrated_brier_score_survival, integrated_brier_score_incidence\n",
    "from lifelines import AalenJohansenFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.models import DeepHit\n",
    "from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis, RandomSurvivalForest\n",
    "from sksurv.metrics import check_y_survival, CensoringDistributionEstimator\n",
    "from sksurv.metrics import cumulative_dynamic_auc\n",
    "from sksurv.util import Surv\n",
    "from types import NoneType\n",
    "from typing import List\n",
    "from zipfile import ZipFile\n",
    "import copy\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mUZ1yrf9_1N"
   },
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UyEjRf0QOavh"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Kb4cW5oacgld"
   },
   "outputs": [],
   "source": [
    "dtypes = {'credit_score': 'Int16', 'first_payment_date': 'str', 'first_time_homebuyer_flag': 'str',\n",
    "        'maturity_date': 'str', 'MSA': 'Int32', 'MI_%': 'Int16', 'units_numb': 'Int8', 'occupancy_status': 'str',\n",
    "        'CLTV': 'Int16', 'DTI_ratio': 'Int16', 'orig_UPB': 'Int64', 'LTV': 'Int16', 'orig_interest_rate': 'str',\n",
    "        'channel': 'str', 'PPM_flag': 'str', 'amortization_type': 'str',\n",
    "        'property_state': 'str', 'property_type': 'str', 'postal_code': 'Int32', 'id_loan': 'str',\n",
    "        'loan_purpose': 'str', 'orig_loan_term': 'Int16', 'borrowers_num': 'Int8', 'seller_name': 'str',\n",
    "        'service_name': 'str', 'super_conf_flag': 'str', 'id_loan_preharp': 'str',\n",
    "        'program_ind': 'str', 'HARP_ind': 'str', 'property_val_method': 'Int64',\n",
    "        'int_only_flag': 'str', 'MI_cancel_flag': 'str', 'orig_interest_rate':'float32'}\n",
    "\n",
    "static = ['credit_score', 'first_time_homebuyer_flag', 'units_numb', 'MSA', 'MI_%', 'occupancy_status', 'CLTV', 'DTI_ratio', 'orig_UPB',\n",
    "       'LTV', 'orig_interest_rate', 'channel', 'PPM_flag', 'amortization_type',\n",
    "       'property_state', 'property_type', 'loan_purpose', 'orig_loan_term', 'borrowers_num', 'super_conf_flag',\n",
    "       'int_only_flag', 'property_val_method']\n",
    "\n",
    "categ = ['occupancy_status', 'first_time_homebuyer_flag', 'channel', 'PPM_flag', 'amortization_type',\n",
    "       'property_state', 'borrowers_num', 'int_only_flag', 'property_val_method', 'modification_flag', 'step_mod_flag', 'deferred_payment_plan',\n",
    "         'ELTV', 'delinq_due_disaster', 'borrowe_asistance_stat_code', 'property_type', 'loan_purpose', 'super_conf_flag']\n",
    "\n",
    "def get_y(cens, time):\n",
    "    cens, time = np.array(cens), np.array(time)\n",
    "    y = np.empty(dtype=[('event', int), ('duration', np.float64)], shape=cens.shape[0])\n",
    "    y['event'] = cens\n",
    "    y['duration'] = time\n",
    "    return y\n",
    "\n",
    "def get_y_arr(y):\n",
    "    cens, time = np.array(y.event), np.array(y.duration)\n",
    "    y = np.empty(dtype=[('event', bool), ('duration', np.float64)], shape=cens.shape[0])\n",
    "    y['event'] = cens\n",
    "    y['duration'] = time\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_y_arr_int(y):\n",
    "    cens, time = np.array(y.event), np.array(y.duration)\n",
    "    y = np.empty(dtype=[('event', int), ('duration', np.float64)], shape=cens.shape[0])\n",
    "    y['event'] = cens\n",
    "    y['duration'] = time\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_y_event(y_, events: List=[]):\n",
    "    y = np.empty(dtype=[('event', bool), ('duration', np.float64)], shape=y_.shape[0])\n",
    "    y['event'] = y_.event.isin(events)\n",
    "    y['duration'] = y_.duration\n",
    "    return y\n",
    "\n",
    "def case1(X_, y_, events: List=[]):\n",
    "    y, X = y_.copy(), X_.copy()\n",
    "    mask = y_.event.isin(events)\n",
    "    y.event = mask.astype('int')\n",
    "    return X, y\n",
    "\n",
    "def case2(X_, y_, events: List=[]):\n",
    "    y, X = y_.copy(), X_.copy()\n",
    "    mask = y_.event.isin(events)\n",
    "    y.event = mask\n",
    "    y = y[mask]\n",
    "    X = X[mask]\n",
    "    return X, y\n",
    "\n",
    "def case3(X_, y_, events: List=[], ):\n",
    "    y, X = y_.copy(), X_.copy()\n",
    "    mask = y_.event.isin(events)\n",
    "    maxm = y_.duration[mask].max()\n",
    "    y.event = mask\n",
    "    X = X[y.duration<=maxm]\n",
    "    y = y[y.duration<=maxm]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def transform_timegrid(curves, time, grid):\n",
    "    if time.max() < grid.max():\n",
    "        time = np.hstack([time, np.array([grid.max()+1])])\n",
    "        if len(curves.shape)==1:\n",
    "            curves = np.hstack([curves, np.array([0])])\n",
    "        elif len(curves.shape)==2:\n",
    "            curves = np.hstack([curves, np.zeros(shape=(curves.shape[0], 1))])\n",
    "    ind = np.searchsorted(time, grid)\n",
    "    if len(curves.shape)==1:\n",
    "      return curves[ind]\n",
    "    elif len(curves.shape)==2:\n",
    "      return curves[:, ind]\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "\n",
    "def transform_curves(curves): # noninc\n",
    "  \"\"\"\n",
    "    - curves - current survvial_function(s)\n",
    "    -------\n",
    "    Returns:\n",
    "    - array of nonincreasing survival function(s)\n",
    "  \"\"\"\n",
    "  if len(curves.shape) == 1: curves = curves[None, :]\n",
    "  return np.array(list(map(lambda tmp:\n",
    "                           reduce(lambda c, x: (c[0], c[1]+[c[0]])\n",
    "                           if x > c[0] else (x, c[1]+[x]),\n",
    "                           tmp[1:], (tmp[0], [tmp[0]]))[1],\n",
    "                       curves)))\n",
    "\n",
    "def transform_events(y):\n",
    "  events = sorted(y.event.unique())\n",
    "  d = {events[i]:i for i in range(len(events))}\n",
    "  return y.replace({\"event\": d}), d\n",
    "\n",
    "def step_to_array(step_functions):\n",
    "    shape_=(step_functions.shape[0], step_functions[0].x.shape[0])\n",
    "    arr = np.empty(shape=shape_)\n",
    "    for i in range(len(step_functions)):\n",
    "        arr[i] = step_functions[i].y\n",
    "    return arr, step_functions[0].x\n",
    "\n",
    "\n",
    "def str_to_categ(df_col):\n",
    "    uniq = df_col.unique()\n",
    "    return df_col.map(dict(zip(uniq, range(len(uniq)))))\n",
    "\n",
    "\n",
    "class Scaler():\n",
    "    def __init__(self):\n",
    "        self.constant_cols=['int_only_flag', 'property_val_method', 'super_conf_flag', 'amortization_type']\n",
    "        self.categs = list((set(static) & set(categ) - set(self.constant_cols)))\n",
    "        self.enc = ColumnTransformer(transformers=[('ohe', OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\"), self.categs)], remainder='passthrough')\n",
    "   \n",
    "    def fit(self, list_of_df):\n",
    "        X = pd.concat(list_of_df, axis=0)\n",
    "        X.drop(self.constant_cols, inplace=True, axis=1)\n",
    "        self.enc.fit(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X.MSA.fillna(X.MSA.median(), inplace=True)\n",
    "        X.drop(self.constant_cols, inplace=True, axis=1)\n",
    "        X = self.enc.transform(X)  \n",
    "        scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "        X = scaler.fit_transform(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "def bal280_sample():\n",
    "    file = r\"D:\\SurvivalAnalysis\\CompetingRisk\\result_280bal.csv\"\n",
    "    df = pd.read_csv(file, dtype=dtypes)\n",
    "    df['event'] = df.zero_balance_code.astype('int')*(df.cens.astype('int'))\n",
    "    df = df[static+['time', 'event']]\n",
    "    df = df.apply(lambda x: str_to_categ(x) if x.name in categ else x, axis=0)\n",
    "    sign = sorted(list(set(df.columns) - {'time', 'event'}))\n",
    "    y = get_y(df['event'], df['time'] + 1)\n",
    "    X = df.loc[:, sign]\n",
    "    return y, X, sign, categ, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_large, x_large, _, _, _ = bal280_sample()\n",
    "sc = Scaler()\n",
    "sc.fit([x_large])\n",
    "x_large = sc.transform(x_large)\n",
    "\n",
    "y_large = pd.DataFrame(y_large)\n",
    "y_large, dct = transform_events(y_large)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_large, y_large, test_size=0.2, stratify=y_large.event, random_state = 1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, stratify=y_train.event, random_state = 1)\n",
    "TIME_GRID = np.linspace(y_train['duration'].min(), y_train['duration'].max(), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2cPFhC2_KaN"
   },
   "source": [
    "# Модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1NlUE8O_Xvs"
   },
   "source": [
    "## AalenJohansenFitter (lifelines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AalenJohansenModel(y_train, y_test):\n",
    "    events = sorted(y_train.event.unique())\n",
    "    ibs = np.empty(shape=(len(y_train.event.unique())-1,))\n",
    "    aj = AalenJohansenFitter()\n",
    "    for i in events:\n",
    "      if i>0:\n",
    "#         print(i)\n",
    "        T = y_train.duration[y_train.event.isin([i])]\n",
    "        E = y_train.event[y_train.event.isin([i])]\n",
    "        aj.fit(T, E, event_of_interest=i)\n",
    "        curve = 1-aj.cumulative_density_[f'CIF_{i}']\n",
    "        new_curve = transform_timegrid(curve.values, aj.cumulative_density_.index, TIME_GRID).T.astype(float)\n",
    "\n",
    "        ibs_self = ibs_remain(\n",
    "                        get_y_self_event(y_train, [i]),\n",
    "                        get_y_self_event(y_test, [i]),\n",
    "                        np.repeat(new_curve[np.newaxis, :], y_test.shape[0], axis=0)[y_test.event == i, :],\n",
    "                        times=TIME_GRID, axis=-1)\n",
    "        ibs[i-1] = ibs_self\n",
    "#         print(ibs_self)\n",
    "    return ibs\n",
    "#         print(f\"ibs {i:<2}:   {ibs_self:<4.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CKd-Zn1C8Pa"
   },
   "source": [
    "## SurvivalBoost (hazardous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SurvivalBoostModel(x_train, y_train, x_test, y_test):\n",
    "    ibs = np.empty(shape=(2, len(y_train.event.unique())-1))\n",
    "    survival_boost = SurvivalBoost(show_progressbar=False).fit(x_train, y_train)\n",
    "    cif_sb = survival_boost.predict_cumulative_incidence(x_test, times=TIME_GRID)\n",
    "    for i in sorted(y_test.event.unique()):\n",
    "      if i:\n",
    "        sf = 1-cif_sb[:, i, :]\n",
    "        ibs_old =  ibs_remain(get_y_self_event(y_train, [i]),\n",
    "                           get_y_self_event(y_test, [i]), \n",
    "                           sf[y_test.event == i, :],\n",
    "                           times=TIME_GRID, axis=-1)\n",
    "        ibs[0][i-1] = ibs_old\n",
    "        sf = transform_curves(sf)\n",
    "        ibs_noninc =  ibs_remain(get_y_self_event(y_train, [i]),\n",
    "                           get_y_self_event(y_test, [i]),\n",
    "                           sf[y_test.event == i, :],\n",
    "                           times=TIME_GRID, axis=-1)\n",
    "        ibs[1][i-1] = ibs_noninc\n",
    "    return ibs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZ3jHCaPDH2b"
   },
   "source": [
    "## DeepHit (pycox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.models import DeepHit\n",
    "\n",
    "class DeepHitWrapper:\n",
    "\n",
    "    class LabTransform(LabTransDiscreteTime):\n",
    "        def transform(self, durations, events):\n",
    "            durations, is_event = super().transform(durations, events > 0)\n",
    "            events[is_event == 0] = 0\n",
    "            return durations, events.astype('int64')\n",
    "\n",
    "\n",
    "    class CauseSpecificNet(torch.nn.Module):\n",
    "        \"\"\"Network structure similar to the DeepHit paper, but without the residual\n",
    "        connections (for simplicity).\n",
    "        \"\"\"\n",
    "        def __init__(self, in_features, num_nodes_shared, num_nodes_indiv, num_risks,\n",
    "                     out_features, batch_norm=True, dropout=None):\n",
    "            super().__init__()\n",
    "            self.shared_net = tt.practical.MLPVanilla(\n",
    "                in_features, num_nodes_shared[:-1], num_nodes_shared[-1],\n",
    "                batch_norm, dropout,\n",
    "            )\n",
    "            self.risk_nets = torch.nn.ModuleList()\n",
    "            for _ in range(num_risks):\n",
    "                net = tt.practical.MLPVanilla(\n",
    "                    num_nodes_shared[-1], num_nodes_indiv, out_features,\n",
    "                    batch_norm, dropout,\n",
    "                )\n",
    "                self.risk_nets.append(net)\n",
    "\n",
    "        def forward(self, input):\n",
    "            out = self.shared_net(input)\n",
    "            out = [net(out) for net in self.risk_nets]\n",
    "            out = torch.stack(out, dim=1)\n",
    "            return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_x(df):\n",
    "        return df.values.astype('float32')\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_target(df):\n",
    "        return df['duration'].values, df['event'].values\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.labtrans = DeepHitWrapper.LabTransform(100)\n",
    "        self.best_params_ = {'sigma': 0.3, 'num_nodes_shared': [64, 64],\n",
    "                            'num_nodes_indiv': [32], 'lr': 0.001, 'dropout': 0.3, \n",
    "                            'decoupled_weight_decay': 0.01, 'batch_size': 512,\n",
    "                            'batch_norm': False, 'alpha': 0.2}\n",
    "        self.model=None\n",
    "        self.events = None\n",
    "        self.TIME_GRID=None\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val):\n",
    "        self.events = y_train.event.unique()\n",
    "        self.TIME_GRID = np.linspace(y_train['duration'].min(), y_train['duration'].max(), 100)\n",
    "        \n",
    "        y_train_dh = self.labtrans.fit_transform(*self._get_target(y_train.sort_values(by='duration')))\n",
    "        y_val_dh = self.labtrans.transform(*self._get_target(y_val))\n",
    "        x_train_dh = self._get_x(x_train)\n",
    "        x_val_dh = self._get_x(x_val)\n",
    "\n",
    "        val = (x_val_dh, y_val_dh)\n",
    "        train = (x_train_dh, y_train_dh)\n",
    "        \n",
    "        in_features = x_train_dh.shape[1]\n",
    "        num_nodes_shared = self.best_params_['num_nodes_shared']\n",
    "        num_nodes_indiv = self.best_params_['num_nodes_indiv']\n",
    "        num_risks = y_train_dh[1].max()\n",
    "        out_features = len(self.labtrans.cuts)\n",
    "        batch_norm = self.best_params_['batch_norm']\n",
    "        dropout = self.best_params_['dropout']\n",
    "\n",
    "\n",
    "        net = DeepHitWrapper.CauseSpecificNet(in_features, num_nodes_shared, num_nodes_indiv, num_risks,\n",
    "                               out_features, batch_norm, dropout)\n",
    "\n",
    "\n",
    "        optimizer = tt.optim.AdamWR(lr=self.best_params_['lr'],\n",
    "                                    decoupled_weight_decay=self.best_params_['decoupled_weight_decay'],\n",
    "                                    cycle_eta_multiplier=0.8)\n",
    "\n",
    "        self.model = DeepHit(net, optimizer, \n",
    "                        alpha=self.best_params_['alpha'], \n",
    "                        sigma=self.best_params_['sigma'], \n",
    "                        duration_index=self.labtrans.cuts)\n",
    "\n",
    "        epochs = 512\n",
    "        batch_size = self.best_params_['batch_size']\n",
    "        callbacks = [tt.callbacks.EarlyStoppingCycle()]\n",
    "        verbose = False\n",
    "\n",
    "        log = self.model.fit(x_train_dh, y_train_dh, batch_size, epochs, callbacks, verbose, val_data=val)\n",
    "        \n",
    "        \n",
    "        \n",
    "       # cif_dh = model.predict_cif(x_test_dh)\n",
    "    \n",
    "    def transform_timegrid(self, curves, time, grid):\n",
    "        if time.max() < grid.max():\n",
    "            time = np.hstack([time, np.array([grid.max()+1])])\n",
    "            if len(curves.shape)==1:\n",
    "                curves = np.hstack([curves, np.array([0])])\n",
    "            elif len(curves.shape)==2:\n",
    "                curves = np.hstack([curves, np.zeros(shape=(curves.shape[0], 1))])\n",
    "        ind = np.searchsorted(time, grid)\n",
    "        if len(curves.shape)==1:\n",
    "            return curves[ind]\n",
    "        elif len(curves.shape)==2:\n",
    "            return curves[:, ind]\n",
    "        else:\n",
    "            return None\n",
    "       \n",
    "\n",
    "    def predict(self, x_test):\n",
    "        self.events = self.events[self.events > 0]\n",
    "        x_test_dh = self._get_x(x_test)\n",
    "        cif = np.moveaxis((1 - self.model.predict_cif(x_test_dh)), (0, 1, 2), (0, 2, 1))\n",
    "        new = np.zeros(shape=(self.events.shape[0], x_test.shape[0], 100))\n",
    "        for i in self.events:\n",
    "            new[i-1] = self.transform_timegrid(cif[i-1], self.model.duration_index, self.TIME_GRID)\n",
    "        return new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepHitModel(x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "    ibs = np.empty(shape=(len(y_train.event.unique())-1, ))\n",
    "    dh = DeepHitWrapper()\n",
    "    dh.fit(x_train, y_train, x_val, y_val)\n",
    "    cif = dh.predict(x_test)\n",
    "    for i in y_train.event.unique():\n",
    "      if i:\n",
    "        sf1 = transform_timegrid((1-cif)[i-1], dh.model.duration_index, TIME_GRID)\n",
    "\n",
    "        ibs_ = ibs_remain(get_y_self_event(y_train, [i]),\n",
    "                          get_y_self_event(y_test, [i]),\n",
    "                          sf1[y_test.event == i, :],\n",
    "                          times=TIME_GRID, axis=-1)\n",
    "        ibs[i-1] = ibs_\n",
    "    return ibs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHV7glDLi7Mt"
   },
   "source": [
    "## Классы-обертки (OvR, MetaModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yvtWte1ti7Mt"
   },
   "outputs": [],
   "source": [
    "class OvR:\n",
    "    def __init__(self, estimator, mode, early_threshold=1.0):\n",
    "        self.estimator = estimator\n",
    "        self.mode = mode       # early, all, single\n",
    "        self.models = None\n",
    "        self.events = None\n",
    "        self.TIME_GRID = None\n",
    "        self.thrsh = early_threshold\n",
    "\n",
    "    def step_to_array(self, step_functions):\n",
    "        shape_=(step_functions.shape[0], step_functions[0].x.shape[0])\n",
    "        arr = np.empty(shape=shape_)\n",
    "        for i in range(len(step_functions)):\n",
    "            arr[i] = step_functions[i].y\n",
    "        return arr, step_functions[0].x\n",
    "\n",
    "    def transform_timegrid(self, curves, time, grid):\n",
    "        if time.max() < grid.max():\n",
    "            time = np.hstack([time, np.array([grid.max()+1])])\n",
    "            if len(curves.shape)==1:\n",
    "                curves = np.hstack([curves, np.array([0])])\n",
    "            elif len(curves.shape)==2:\n",
    "                curves = np.hstack([curves, np.zeros(shape=(curves.shape[0], 1))])\n",
    "        ind = np.searchsorted(time, grid)\n",
    "        if len(curves.shape)==1:\n",
    "            return curves[ind]\n",
    "        elif len(curves.shape)==2:\n",
    "            return curves[:, ind]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def transform_xy(self, X, y, events: List=[], event_of_interest=None):\n",
    "        if self.mode == 'all':\n",
    "            X, y = self.case1(X, y, events)\n",
    "        elif self.mode == 'single':\n",
    "            X, y = self.case2(X, y, events)\n",
    "        elif self.mode == 'early':\n",
    "            X, y = self.case3(X, y, events, early_threshold=self.thrsh)\n",
    "        elif self.mode == 'mix':\n",
    "            if event_of_interest in {5, 6}:\n",
    "                X, y = self.case3(X, y, events, early_threshold=self.thrsh)\n",
    "            else:\n",
    "                X, y = self.case2(X, y, events)\n",
    "        else:\n",
    "            raise ValueError('Wrong mode')\n",
    "        return X, y\n",
    "\n",
    "    def case1(self, X_, y_, events: List=[]):\n",
    "        y, X = y_.copy(), X_.copy()\n",
    "        mask = y_.event.isin(events)\n",
    "        y.event = mask.astype('int')\n",
    "        return X, y\n",
    "\n",
    "    def case2(self, X_, y_, events: List=[]):\n",
    "        y, X = y_.copy(), X_.copy()\n",
    "        mask = y_.event.isin(events)\n",
    "        y.event = mask\n",
    "        y = y[mask]\n",
    "        X = X[mask]\n",
    "        y.event = y.event.astype('int')\n",
    "        return X, y\n",
    "\n",
    "    def case3(self, X_, y_, events: List=[], early_threshold=1.0):\n",
    "        y, X = y_.copy(), X_.copy()\n",
    "        mask = y_.event.isin(events)\n",
    "        maxm = y_.duration[mask].quantile(early_threshold)\n",
    "        y.event = mask\n",
    "        X = X[y.duration<=maxm]\n",
    "        y = y[y.duration<=maxm]\n",
    "        y.event = y.event.astype('int')\n",
    "        return X, y\n",
    "\n",
    "    def fit(self, X_, y_):\n",
    "        self.TIME_GRID = np.linspace(y_['duration'].min(), y_['duration'].max(), 100)\n",
    "        self.model = []\n",
    "        self.events = y_.event.unique()\n",
    "        for i in sorted(self.events):\n",
    "            if i:\n",
    "                tmp = copy.deepcopy(self.estimator)\n",
    "                X, y = self.transform_xy(X_, y_, [i], i)\n",
    "                if type(self.estimator) == CoxPHFitter:\n",
    "                    X = X.join(y)\n",
    "                    tmp.fit(X, duration_col='duration', event_col = 'event')\n",
    "                elif isinstance(self.estimator, AalenJohansenFitter):\n",
    "                    T, E = y.duration, y.event\n",
    "                    tmp.fit(T, E, event_of_interest=1)\n",
    "                else:\n",
    "                    y = get_y_arr(y)\n",
    "                    tmp.fit(X, y)\n",
    "                self.model.append(tmp)\n",
    "\n",
    "    def predict(self, X_):\n",
    "        \"\"\"\n",
    "        shape (6, 4800, 100)\n",
    "        \n",
    "        \"\"\"\n",
    "        predictions = np.empty(shape=(len(self.events)-1, X_.shape[0], 100))\n",
    "        for i in sorted(self.events):\n",
    "            if i:\n",
    "                if type(self.estimator) == CoxPHFitter:\n",
    "                    sf = (self.model[i-1].predict_survival_function(X_, times=TIME_GRID)).T\n",
    "                elif isinstance(self.estimator, AalenJohansenFitter):\n",
    "                    sf = 1-self.model[i-1].cumulative_density_[f'CIF_1']\n",
    "                    sf = transform_timegrid(sf.values, self.model[i-1].cumulative_density_.index, TIME_GRID).T.astype(float)\n",
    "                    sf = np.repeat(sf[np.newaxis, :], X_.shape[0], axis=0)\n",
    "                else:\n",
    "                    sf = self.model[i-1].predict_survival_function(X_)\n",
    "                    sf, times = step_to_array(sf)\n",
    "                    sf = transform_timegrid(sf, times, TIME_GRID)\n",
    "                predictions[i-1]=sf\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SurvivalBoost:\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bOiw43N_i7Mt"
   },
   "outputs": [],
   "source": [
    "class MetaModel:\n",
    "    def __init__(self, estimator, mode='weighted'):\n",
    "        self.ovr = estimator\n",
    "        self.meta_model = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "        self.mode = mode\n",
    "\n",
    "    def fit(self, X_, y_, X_val=None, y_val=None):\n",
    "        if (not (X_val is None)): self.ovr.fit(X_, y_, X_val, y_val)\n",
    "        else: self.ovr.fit(X_, y_)\n",
    "        mask = y_.event>0\n",
    "        self.meta_model.fit(X_[mask], y_.event[mask])\n",
    "\n",
    "    def predict(self, X_):\n",
    "        all_preds = self.ovr.predict(X_) # (7, 4800, 100)\n",
    "        if self.mode == 'best':\n",
    "            events_pred = self.meta_model.predict(X_)\n",
    "            selected_preds = np.zeros((X_.shape[0], 100)) # (4800, 100)\n",
    "            for i in range(X_.shape[0]):\n",
    "                selected_preds[i] = all_preds[events_pred[i]-1, i, :]\n",
    "            return selected_preds\n",
    "        elif self.mode == 'weighted':\n",
    "            events_pred = self.meta_model.predict_proba(X_)\n",
    "            tmp = np.moveaxis(all_preds, (0, 1, 2), (1, 0, 2))\n",
    "            return np.sum(tmp * events_pred[..., np.newaxis], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3POXuI7Vi7Mt"
   },
   "outputs": [],
   "source": [
    "def get_y_self_event(y_, events: List=[]):\n",
    "    l = sum(y_.event.isin(events))\n",
    "    y = np.empty(dtype=[('event', bool), ('duration', np.float64)], shape=l)\n",
    "    y[\"duration\"] = y_[y_.event.isin(events)][\"duration\"]\n",
    "    y[\"event\"] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IBS competrisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# новая версия (исправлено 1/(K+1))\n",
    "def ibs_remain_crk(survival_train, survival_test, estimate, times, axis=-1):\n",
    "    test_event, test_time = survival_test['event'], survival_test['duration']\n",
    "    estimate = np.array(estimate)\n",
    "    estimate = np.moveaxis(estimate, (0, 1, 2), (1, 0, 2))\n",
    "    times = np.array(times)\n",
    "    n_samples = estimate.shape[0]\n",
    "    K = estimate.shape[1]\n",
    "    n_times = estimate.shape[2]\n",
    "    estimate[estimate == -np.inf] = 0\n",
    "    estimate[estimate == np.inf] = 0\n",
    "    \n",
    "    before = times[np.newaxis, :] < test_time[:, np.newaxis]\n",
    "    after_event = np.zeros((n_samples, K, n_times), dtype=bool)\n",
    "    \n",
    "    for k in range(K):\n",
    "        after_event[:, k, :] = (~before) & ((test_event == k+1)[:, np.newaxis])\n",
    "    \n",
    "    estim_before_sq = (1 - estimate) ** 2\n",
    "    estim_after_sq = estimate ** 2\n",
    "    \n",
    "    brier_scores = np.where(\n",
    "        before[:, np.newaxis, :],\n",
    "        estim_before_sq,\n",
    "        np.where(after_event, estim_after_sq, 0))\n",
    "    \n",
    "    brier_scores_n = np.where(\n",
    "        before,\n",
    "        np.sum(brier_scores, axis=1) / (K + 1),\n",
    "        np.sum(brier_scores, axis=1))\n",
    "    \n",
    "    N = np.sum(np.array([np.where(test_time < t, test_event, 1) for i, t in enumerate(times)]), axis=1)\n",
    "    brier_scores = np.sum(brier_scores_n, axis=0)\n",
    "    brier_scores = np.where(N > 0, brier_scores / N, 0)\n",
    " \n",
    "    time_diff = times[-1] - times[0] if times[-1] > times[0] else 1\n",
    "    \n",
    "    if axis == -1: \n",
    "        return np.trapz(brier_scores, times) / time_diff\n",
    "    elif axis == 0: \n",
    "        return np.trapz(brier_scores_n, times, axis=1) / time_diff\n",
    "    elif axis == 1:\n",
    "        return brier_scores\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IBS competrisk balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibs_remain_crk_balanced(survival_train, survival_test, estimate, times, axis=-1):\n",
    "    test_event, test_time = survival_test['event'], survival_test['duration']\n",
    "    estimate = np.array(estimate)\n",
    "    estimate = np.moveaxis(estimate, (0, 1, 2), (1, 0, 2))\n",
    "    times = np.array(times)\n",
    "    \n",
    "    n_samples = estimate.shape[0]\n",
    "    K = estimate.shape[1]\n",
    "    n_times = estimate.shape[2]\n",
    "    \n",
    "    estimate[estimate == -np.inf] = 0\n",
    "    estimate[estimate == np.inf] = 0\n",
    "    \n",
    "    before = times[np.newaxis, :] < test_time[:, np.newaxis]\n",
    "    \n",
    "    estim_before_sq = (1 - estimate) ** 2 \n",
    "    estim_after_sq = estimate ** 2\n",
    "\n",
    "    delta_indices = test_event.astype(int) - 1  # (n_samples,) : shift 1->0, 2->1, etc.\n",
    "    valid_event = test_event > 0\n",
    "    vals = np.zeros((n_samples, n_times))\n",
    "    vals[valid_event] = estim_before_sq[\n",
    "        np.arange(n_samples)[valid_event, np.newaxis], \n",
    "        delta_indices[valid_event, np.newaxis], \n",
    "        np.arange(n_times)\n",
    "    ]\n",
    "    \n",
    "    all_sum = np.sum(estim_before_sq, axis=1)  # (n_samples, n_times)\n",
    "    other_types = np.where(valid_event[:, np.newaxis], \n",
    "                               all_sum - vals,\n",
    "                               all_sum)\n",
    "    bs_before = 0.5 * vals + 0.5 * other_types / K\n",
    "    vals_after = np.zeros((n_samples, n_times))\n",
    "\n",
    "    vals_after[valid_event] = estim_after_sq[\n",
    "        np.arange(n_samples)[valid_event, np.newaxis], \n",
    "        delta_indices[valid_event, np.newaxis], \n",
    "        np.arange(n_times)\n",
    "    ]\n",
    "    \n",
    "    bs_after = vals_after * (~before) * (valid_event > 0)[:, np.newaxis]\n",
    "    bs_obs = np.where(before, bs_before, bs_after)\n",
    "    \n",
    "    N = np.sum(np.array([np.where(test_time < t, test_event, 1)\n",
    "                         for i, t in enumerate(times)]), axis=1)\n",
    "    \n",
    "    brier_scores = np.sum(bs_obs, axis=0)\n",
    "    brier_scores = np.where(N > 0, brier_scores / N, 0)\n",
    "    \n",
    "    time_diff = times[-1] - times[0] if times[-1] > times[0] else 1\n",
    "    \n",
    "    if axis == -1: \n",
    "        return np.trapz(brier_scores, times) / time_diff\n",
    "    elif axis == 0: \n",
    "        return np.trapz(bs_obs, times, axis=1) / time_diff\n",
    "    elif axis == 1:\n",
    "        return brier_scores\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WyPaJDqwJiP3"
   },
   "outputs": [],
   "source": [
    "def ibs_remain(survival_train, survival_test, estimate, times, axis=-1):\n",
    "    \"\"\" IBS with equal impact of partial observation with controlled quantity \"\"\"\n",
    "    test_event, test_time = check_y_survival(survival_test, allow_all_censored=True)\n",
    "    estimate = np.array(estimate)\n",
    "    if estimate.ndim == 1 and times.shape[0] == 1:\n",
    "        estimate = estimate.reshape(-1, 1)\n",
    "    estimate[estimate == -np.inf] = 0\n",
    "    estimate[estimate == np.inf] = 0\n",
    "\n",
    "    estim_before = np.square(estimate) * test_event[np.newaxis, :].T\n",
    "    estim_after = np.square(1 - estimate)\n",
    "    brier_scores = np.array([np.where(test_time < t,\n",
    "                                      estim_before[:, i],\n",
    "                                      estim_after[:, i])\n",
    "                             for i, t in enumerate(times)])\n",
    "    N = np.sum(np.array([np.where(test_time < t, test_event, 1)\n",
    "                         for i, t in enumerate(times)]), axis=1)\n",
    "    time_diff = times[-1] - times[0] if times[-1] > times[0] else 1\n",
    "    if axis == -1:  # mean ibs for each time and observation\n",
    "        # brier_scores = np.mean(brier_scores, axis=1)\n",
    "        brier_scores = np.where(N > 0, 1 / N, 0) * np.sum(brier_scores, axis=1)\n",
    "        return np.trapz(brier_scores, times) / time_diff\n",
    "    elif axis == 0:  # ibs for each observation\n",
    "        return np.trapz(brier_scores, times, axis=0) / time_diff\n",
    "    elif axis == 1:  # bs in time (for graphics)\n",
    "        # brier_scores = np.mean(brier_scores, axis=1)\n",
    "        brier_scores = np.where(N > 0, 1 / N, 0) * np.sum(brier_scores, axis=1)\n",
    "        return brier_scores\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### AUPRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_survival_auprc_vectorized(y_true, preds, time_points, num_phi_steps=100):\n",
    "   \n",
    "    n_obs = preds.shape[0]\n",
    "    ext_time_points = np.concatenate([[0], time_points])\n",
    "    ext_probs = np.concatenate([np.ones((n_obs, 1)), preds], axis=1)\n",
    "    ext_probs = np.minimum.accumulate(ext_probs, axis=1)\n",
    "    true_times = y_true['duration'].reshape(-1, 1)\n",
    "    is_event = y_true['event'].astype(bool).reshape(-1, 1)\n",
    "    true_times[true_times == 0] = 1e-8\n",
    "    phi_grid = np.linspace(0, 1, num_phi_steps).reshape(1, -1)\n",
    "    t_early_matrix = true_times * phi_grid\n",
    "    phi_grid_safe = np.copy(phi_grid)\n",
    "    phi_grid_safe[phi_grid_safe == 0] = 1e-8\n",
    "    t_late_matrix = true_times / phi_grid_safe\n",
    "    t_late_matrix[:, phi_grid[0] == 0] = np.inf\n",
    "    \n",
    "    indices_early = np.searchsorted(ext_time_points, t_early_matrix, side='right') - 1\n",
    "    indices_late = np.searchsorted(ext_time_points, t_late_matrix, side='right') - 1\n",
    "    row_indexer = np.arange(n_obs).reshape(-1, 1)\n",
    "\n",
    "    s_early = ext_probs[row_indexer, indices_early]\n",
    "    s_late = ext_probs[row_indexer, indices_late]\n",
    "    \n",
    "    integral_values = np.where(is_event, s_early - s_late, s_early)\n",
    "    \n",
    "    auprc_scores = np.sum(integral_values, axis=1) / num_phi_steps\n",
    "    \n",
    "    return np.mean(auprc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_threshold = 0.99\n",
    "col = [f'тип {i}' for i in [1, 2, 3, 9, 15, 16, 96]]\n",
    "ind = [f'OVR({i}) - {j}' for i in ['Cox', 'RSF'] for j in ['single', 'all', 'early']]\n",
    "ovrs = [OvR(CoxPHFitter(penalizer=0.02), mode='single', early_threshold=early_threshold),\n",
    "        OvR(CoxPHFitter(penalizer=0.02), mode='all', early_threshold=early_threshold),\n",
    "        OvR(CoxPHFitter(penalizer=0.02), mode='early', early_threshold=early_threshold),\n",
    "\n",
    "        OvR(RandomSurvivalForest(n_jobs=-1, random_state=42), mode='single', early_threshold=early_threshold),\n",
    "        OvR(RandomSurvivalForest(n_jobs=-1, random_state=42), mode='all', early_threshold=early_threshold),\n",
    "        OvR(RandomSurvivalForest(n_jobs=-1, random_state=42), mode='early', early_threshold=early_threshold)]\n",
    "\n",
    "for elem in ovrs:\n",
    "    elem.fit(x_train, y_train)\n",
    "\n",
    "preds = [model.predict(x_test) for model in ovrs]\n",
    "ovr_ibs = np.empty(shape=(7, len(preds)))\n",
    "ovr_auc = np.empty(shape=(7, len(preds)))\n",
    "\n",
    "\n",
    "for i in y_train.event.unique():\n",
    "  if i:\n",
    "    for j, cif in enumerate(preds):\n",
    "        sf = cif[i-1, ...]    \n",
    "        ovr_auc[i-1][j] = calculate_survival_auprc_vectorized(\n",
    "            get_y_self_event(y_test, [i]), \n",
    "            sf[y_test.event == i, :],\n",
    "            TIME_GRID)\n",
    "        ovr_ibs[i-1][j] = ibs_remain(get_y_self_event(y_train, [i]),\n",
    "            get_y_self_event(y_test, [i]),\n",
    "            sf[y_test.event == i, :],\n",
    "            times=TIME_GRID, axis=-1)\n",
    "\n",
    "df_auc = pd.DataFrame(ovr_auc.T, columns=col, index=ind)\n",
    "df_ibs = pd.DataFrame(ovr_ibs.T, columns=col, index=ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_threshold = 0.99\n",
    "col = [f'тип {i}' for i in [1, 2, 3, 9, 15, 16, 96]]\n",
    "ind = [f'Meta_w(OVR({i})) - {j}' for i in ['Cox', 'RSF'] for j in ['single', 'all', 'early']]\n",
    "ovrs = [OvR(CoxPHFitter(penalizer=0.02), mode='single', early_threshold=early_threshold),\n",
    "        OvR(CoxPHFitter(penalizer=0.02), mode='all', early_threshold=early_threshold),\n",
    "        OvR(CoxPHFitter(penalizer=0.02), mode='early', early_threshold=early_threshold),\n",
    "\n",
    "        OvR(RandomSurvivalForest(n_jobs=-1, random_state=42), mode='single', early_threshold=early_threshold),\n",
    "        OvR(RandomSurvivalForest(n_jobs=-1, random_state=42), mode='all', early_threshold=early_threshold),\n",
    "        OvR(RandomSurvivalForest(n_jobs=-1, random_state=42), mode='early', early_threshold=early_threshold)]\n",
    "\n",
    "metas_b = list(map(lambda x: MetaModel(x, mode='best'), ovrs))\n",
    "for m in metas_w:\n",
    "    m.fit(x_train, y_train)\n",
    "    \n",
    "preds = [m.predict(x_test) for m in metas_b]\n",
    "meta_b_ibs = np.empty(shape=(7, len(preds)))\n",
    "meta_b_auc = np.empty(shape=(7, len(preds)))\n",
    "\n",
    "for i in y_train.event.unique():\n",
    "  if i:\n",
    "    for j, cif in enumerate(preds):\n",
    "        # sf = cif[i-1, ...]    \n",
    "        meta_b_auc[i-1][j] = calculate_survival_auprc_vectorized(\n",
    "            get_y_self_event(y_test, [i]), \n",
    "            cif[y_test.event == i, :],\n",
    "            TIME_GRID)\n",
    "        meta_b_ibs[i-1][j] = ibs_remain(get_y_self_event(y_train, [i]),\n",
    "            get_y_self_event(y_test, [i]),\n",
    "            cif[y_test.event == i, :],\n",
    "            times=TIME_GRID, axis=-1)\n",
    "\n",
    "df_ibs = pd.DataFrame(meta_b_ibs.T, columns=col, index=ind)\n",
    "df_auc = pd.DataFrame(meta_b_auc.T, columns=col, index=ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_threshold = 0.99\n",
    "col = [f'тип {i}' for i in [1, 2, 3, 9, 15, 16, 96]]\n",
    "ind = [f'Meta_w(OVR({i})) - {j}' for i in ['Cox', 'RSF'] for j in ['single', 'all', 'early']]\n",
    "ovrs = [OvR(CoxPHFitter(penalizer=0.02), mode='single', early_threshold=early_threshold),\n",
    "        OvR(CoxPHFitter(penalizer=0.02), mode='all', early_threshold=early_threshold),\n",
    "        OvR(CoxPHFitter(penalizer=0.02), mode='early', early_threshold=early_threshold),\n",
    "\n",
    "        OvR(RandomSurvivalForest(n_jobs=-1, random_state=42), mode='single', early_threshold=early_threshold),\n",
    "        OvR(RandomSurvivalForest(n_jobs=-1, random_state=42), mode='all', early_threshold=early_threshold),\n",
    "        OvR(RandomSurvivalForest(n_jobs=-1, random_state=42), mode='early', early_threshold=early_threshold)]\n",
    "\n",
    "metas_w = list(map(lambda x: MetaModel(x, mode='weighted'), ovrs))\n",
    "for m in metas_w:\n",
    "    m.fit(x_train, y_train)\n",
    "    \n",
    "preds = [m.predict(x_test) for m in metas_w]\n",
    "meta_w_ibs = np.empty(shape=(7, len(preds)))\n",
    "meta_w_auc = np.empty(shape=(7, len(preds)))\n",
    "\n",
    "for i in y_train.event.unique():\n",
    "  if i:\n",
    "    for j, cif in enumerate(preds):\n",
    "        # sf = cif[i-1, ...]    \n",
    "        meta_w_auc[i-1][j] = calculate_survival_auprc_vectorized(\n",
    "            get_y_self_event(y_test, [i]), \n",
    "            cif[y_test.event == i, :],\n",
    "            TIME_GRID)\n",
    "        meta_w_ibs[i-1][j] = ibs_remain(get_y_self_event(y_train, [i]),\n",
    "                          get_y_self_event(y_test, [i]),\n",
    "                          cif[y_test.event == i, :],\n",
    "                          times=TIME_GRID, axis=-1)\n",
    "\n",
    "df_ibs = pd.DataFrame(meta_w_ibs.T, columns=col, index=ind)\n",
    "df_auc = pd.DataFrame(meta_w_auc.T, columns=col, index=ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auc"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "A1NlUE8O_Xvs",
    "UTXIHWCL_pav",
    "3Hge-4VDcqiU"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
